experiment:
  # Model and decoding for generation (vLLM)
  qwen_model_id: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B  # model to load in vLLM
  temperature: 0.6                                        # sampling temperature
  max_tokens: 4096                                        # max generation tokens
  qwen_reasoning_parser: null                             # optional reasoning parser (null = none)
  qwen_max_model_len: 4096                                # context window for the model

  # Batch and reproducibility
  actor_batch_size: 2048                                     # vLLM batch size for generation
  random_seed: 42                                         # seed for reproducibility

  # Outputs
  output_dir: results                                     # where to save run outputs

  # Input questions (JSON list of {question, answer})
  questions_path: filtered_gsm8k.questions_lightfiltered.questions.json
